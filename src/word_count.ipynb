{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "Ni6j9jAEn0g5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKIpvRriQVvI"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "import string\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "YdvFCXMhnwNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_word(word: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes punctuation and convert word to lowercase.\n",
        "    \"\"\"\n",
        "    return word.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
        "\n",
        "\n",
        "def save_output(rdd, output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Saves RDD to output path. If the directory exists, deletes it first.\n",
        "    \"\"\"\n",
        "    path = Path(output_path)\n",
        "    if path.exists() and path.is_dir():\n",
        "        print(f\"[INFO] Output path '{output_path}' exists. Deleting it first.\")\n",
        "        shutil.rmtree(path)\n",
        "    rdd.saveAsTextFile(output_path)\n",
        "    print(f\"[INFO] Saved RDD to '{output_path}'\")\n",
        "\n",
        "\n",
        "def print_top_n(rdd, n: int = 25) -> None:\n",
        "    \"\"\"\n",
        "    Prints the top `n` (default is 25) elements from the RDD.\n",
        "    \"\"\"\n",
        "    top_items = rdd.take(n)\n",
        "    for word, count in top_items:\n",
        "        print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "qCUErjhOns1X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Count"
      ],
      "metadata": {
        "id": "WCmOOThJoQ-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize `SparkContext`"
      ],
      "metadata": {
        "id": "aavrAJpUooYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(\"local\", \"WordCount\")"
      ],
      "metadata": {
        "id": "pSaCGtKvoON4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define custom stop words"
      ],
      "metadata": {
        "id": "4WxR5xv3ovlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words: Set[str] = set(\n",
        "    [\n",
        "        \"the\",\n",
        "        \"and\",\n",
        "        \"a\",\n",
        "        \"an\",\n",
        "        \"to\",\n",
        "        \"in\",\n",
        "        \"is\",\n",
        "        \"it\",\n",
        "        \"of\",\n",
        "        \"that\",\n",
        "        \"this\",\n",
        "        \"on\",\n",
        "        \"was\",\n",
        "        \"with\",\n",
        "        \"as\",\n",
        "        \"for\",\n",
        "        \"but\",\n",
        "        \"by\",\n",
        "        \"be\",\n",
        "        \"at\",\n",
        "        \"are\",\n",
        "        \"or\",\n",
        "        \"he\",\n",
        "        \"she\",\n",
        "        \"i\",\n",
        "        \"you\",\n",
        "        \"they\",\n",
        "        \"we\",\n",
        "        \"his\",\n",
        "        \"her\",\n",
        "        \"their\",\n",
        "        \"my\",\n",
        "        \"me\",\n",
        "        \"your\",\n",
        "        \"has\",\n",
        "        \"have\",\n",
        "        \"had\",\n",
        "        \"will\",\n",
        "        \"would\",\n",
        "        \"can\",\n",
        "        \"could\",\n",
        "        \"should\",\n",
        "        \"do\",\n",
        "        \"does\",\n",
        "        \"did\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "oIs1LdvTosq5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read input files"
      ],
      "metadata": {
        "id": "D18vBqABo005"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book1 = sc.textFile(\"book1.txt\")\n",
        "book2 = sc.textFile(\"book2.txt\")\n",
        "full_text = book1.union(book2)"
      ],
      "metadata": {
        "id": "eCyRw0UeozdZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1 - Basic Word Count"
      ],
      "metadata": {
        "id": "UlOAMXADpXi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic_counts = (\n",
        "    full_text\n",
        "    .flatMap(lambda line: line.split())\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "save_output(basic_counts, \"output_1.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iKHJIhlpVqT",
        "outputId": "5afbed8f-bd6f-415d-bb6b-cdddb7d6051f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Output path 'output_1.txt' exists. Deleting it first.\n",
            "[INFO] Saved RDD to 'output_1.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 - Extended Word Count"
      ],
      "metadata": {
        "id": "fy_LKuWrpoyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extended_counts = (\n",
        "    full_text\n",
        "    .flatMap(lambda line: line.split())\n",
        "    .map(lambda word: clean_word(word))\n",
        "    .filter(lambda word: word and word not in stop_words)\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "save_output(extended_counts, \"output_1_extended.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-BfkX2Epj0o",
        "outputId": "cae43d5c-601e-452c-bda9-b7464508a2e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Output path 'output_1_extended.txt' exists. Deleting it first.\n",
            "[INFO] Saved RDD to 'output_1_extended.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.4 - Top 25 from book1.txt only (extended)"
      ],
      "metadata": {
        "id": "wYosVXwKp5PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book1_extended = (\n",
        "    book1\n",
        "    .flatMap(lambda line: line.split())\n",
        "    .map(lambda word: clean_word(word))\n",
        "    .filter(lambda word: word and word not in stop_words)\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nTop 25 words from book1.txt (extended):\")\n",
        "print_top_n(book1_extended, 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZHXXcOpwLl",
        "outputId": "eb6b88bb-8f35-40d9-be6d-be17aae2e4a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 25 words from book1.txt (extended):\n",
            "not: 1505\n",
            "mr: 791\n",
            "him: 725\n",
            "all: 640\n",
            "elizabeth: 599\n",
            "so: 593\n",
            "were: 566\n",
            "which: 565\n",
            "been: 534\n",
            "from: 519\n",
            "very: 492\n",
            "no: 478\n",
            "what: 452\n",
            "them: 420\n",
            "said: 405\n",
            "such: 398\n",
            "when: 370\n",
            "darcy: 358\n",
            "mrs: 349\n",
            "there: 347\n",
            "if: 341\n",
            "more: 335\n",
            "much: 330\n",
            "must: 323\n",
            "am: 322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop `SparkContext`"
      ],
      "metadata": {
        "id": "vb4dPb-jqNir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "04GR6cVoqIS_"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}